{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pFxfngQTYeF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f481ba1-f113-4b5d-ab7e-17748c7cd5c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-text>=2.11\n",
            "  Downloading tensorflow_text-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text>=2.11) (0.15.0)\n",
            "Collecting tensorflow<2.15,>=2.14.0 (from tensorflow-text>=2.11)\n",
            "  Downloading tensorflow-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.8/489.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (16.0.6)\n",
            "Collecting ml-dtypes==0.2.0 (from tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11)\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (4.5.0)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (1.59.0)\n",
            "Collecting tensorboard<2.15,>=2.14 (from tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11)\n",
            "  Downloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11)\n",
            "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.15,>=2.14.0 (from tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11)\n",
            "  Downloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (3.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (3.0.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text>=2.11) (3.2.2)\n",
            "Installing collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow, tensorflow-text\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.15.0\n",
            "    Uninstalling wrapt-1.15.0:\n",
            "      Successfully uninstalled wrapt-1.15.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.13.0\n",
            "    Uninstalling tensorflow-estimator-2.13.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.13.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.3.1\n",
            "    Uninstalling ml-dtypes-0.3.1:\n",
            "      Successfully uninstalled ml-dtypes-0.3.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.13.1\n",
            "    Uninstalling keras-2.13.1:\n",
            "      Successfully uninstalled keras-2.13.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.13.0\n",
            "    Uninstalling tensorboard-2.13.0:\n",
            "      Successfully uninstalled tensorboard-2.13.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.13.0\n",
            "    Uninstalling tensorflow-2.13.0:\n",
            "      Successfully uninstalled tensorflow-2.13.0\n",
            "Successfully installed keras-2.14.0 ml-dtypes-0.2.0 tensorboard-2.14.1 tensorflow-2.14.0 tensorflow-estimator-2.14.0 tensorflow-text-2.14.0 wrapt-1.14.1\n"
          ]
        }
      ],
      "source": [
        "! pip install \"tensorflow-text>=2.11\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pathlib\n",
        "import os\n",
        "import tensorflow_text as tf_text\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers"
      ],
      "metadata": {
        "id": "anRooMNnfvi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_zip = keras.utils.get_file('spa-eng.zip',origin = 'http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',extract = True)\n",
        "path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng'/'spa.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B14AaiklZR7Y",
        "outputId": "27f894b2-e9b9-4d59-d7cb-dc476d115001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2638744/2638744 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path):\n",
        "  text = path.read_text(encoding='utf-8')\n",
        "\n",
        "  lines = text.splitlines()\n",
        "  pairs = [line.split('\\t') for line in lines]\n",
        "\n",
        "  context = np.array([context for target,context in pairs])\n",
        "  target = np.array([target for target,context in pairs])\n",
        "\n",
        "  return target, context"
      ],
      "metadata": {
        "id": "dHw5aEr_abIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_raw,context_raw = load_data(path_to_file)\n",
        "\n",
        "print(context_raw[1])\n",
        "print(target_raw[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qju42OL6b0s2",
        "outputId": "d2a22902-f5b9-4447-d74f-cd4dca0d0a08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vete.\n",
            "Go.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Buffer_Size = len(context_raw)\n",
        "Batch_Size = 64\n",
        "\n",
        "is_train = np.random.uniform(size=(len(target_raw),)) < 0.8\n",
        "\n",
        "train_raw = (tf.data.Dataset\n",
        "    .from_tensor_slices((context_raw[is_train], target_raw[is_train])).shuffle(Buffer_Size).batch(Batch_Size))\n",
        "val_raw = (tf.data.Dataset\n",
        "    .from_tensor_slices((context_raw[~is_train], target_raw[~is_train])).shuffle(Buffer_Size).batch(Batch_Size))"
      ],
      "metadata": {
        "id": "PsumOazdcLQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for example_context_strings, example_target_strings in train_raw.take(1):\n",
        "  print(example_context_strings[:5])\n",
        "  print(example_target_strings[:5])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_15EQ0hdIaG",
        "outputId": "1765ed82-1da2-4d90-83db-023c599d0e53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'Tom ya no lo necesitaba.' b'Tan solo d\\xc3\\xadganle la verdad.'\n",
            " b'No pens\\xc3\\xa9 que Tom quisiera volver a verme.'\n",
            " b'No se puede comprar en ning\\xc3\\xban otro lugar, solo all\\xc3\\xad.'\n",
            " b'Tom salt\\xc3\\xb3 al agua helada.'], shape=(5,), dtype=string)\n",
            "\n",
            "tf.Tensor(\n",
            "[b\"Tom didn't need it anymore.\" b'Just tell him the truth.'\n",
            " b\"I didn't think Tom would want to see me again.\"\n",
            " b\"You can't buy it anywhere but there.\"\n",
            " b'Tom jumped into the cold water.'], shape=(5,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standarization\n",
        "def tf_lower_and_split_punct(text):\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text"
      ],
      "metadata": {
        "id": "kjA70_TudRlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we do a text vectorization\n",
        "\n",
        "max_vocab_size = 10000\n",
        "\n",
        "context_text_processor = layers.TextVectorization(standardize=tf_lower_and_split_punct,\n",
        "                                                  max_tokens=max_vocab_size,\n",
        "                                                  ragged=True)\n",
        "\n",
        "context_text_processor.adapt(train_raw.map(lambda context, target : context))\n",
        "\n",
        "context_text_processor.get_vocabulary()[:15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbZ5PGzC5Qbl",
        "outputId": "7cd8cfab-6729-4f69-843c-d60932f13343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " '[START]',\n",
              " '[END]',\n",
              " '.',\n",
              " 'que',\n",
              " 'de',\n",
              " 'el',\n",
              " 'a',\n",
              " 'no',\n",
              " 'tom',\n",
              " 'la',\n",
              " '?',\n",
              " '¿',\n",
              " 'en']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text vectorization also for target values\n",
        "target_text_processor = layers.TextVectorization(standardize=tf_lower_and_split_punct,\n",
        "                                                  max_tokens=max_vocab_size,\n",
        "                                                  ragged=True)\n",
        "\n",
        "target_text_processor.adapt(train_raw.map(lambda context, target : target))\n",
        "\n",
        "target_text_processor.get_vocabulary()[:15]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ljttfpG6oeT",
        "outputId": "396bca1c-9cd4-4d31-bc24-92378962260c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " '[START]',\n",
              " '[END]',\n",
              " '.',\n",
              " 'the',\n",
              " 'i',\n",
              " 'to',\n",
              " 'you',\n",
              " 'tom',\n",
              " 'a',\n",
              " '?',\n",
              " 'is',\n",
              " 'he',\n",
              " 'in']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_vocab = np.array(context_text_processor.get_vocabulary())"
      ],
      "metadata": {
        "id": "ZP62wrvg7OBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# process the dataset\n",
        "def process_text(context , target):\n",
        "  context = context_text_processor(context).to_tensor()\n",
        "  target = target_text_processor(target)\n",
        "  targ_in = target[:,:-1].to_tensor()\n",
        "  targ_out = target[:,1:].to_tensor()\n",
        "  return (context,targ_in),targ_out\n",
        "\n",
        "train_ds = train_raw.map(process_text,tf.data.AUTOTUNE)\n",
        "val_ds = val_raw.map(process_text, tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "iG_5nUQH8P68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now build the encoder\n",
        "\n",
        "class Encoder(layers.Layer):\n",
        "  def __init__(self,text_processor,units):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.text_processor = text_processor\n",
        "    self.vocab_size = text_processor.vocabulary_size()\n",
        "    self.units = units\n",
        "\n",
        "    self.embedding = layers.Embedding(self.vocab_size,units,mask_zero = True)\n",
        "\n",
        "    self.rnn = layers.Bidirectional(merge_mode = 'sum',\n",
        "                                    layer = layers.GRU(units,return_sequences = True,recurrent_initializer='glorot_uniform'))\n",
        "\n",
        "  def call(self,x):\n",
        "    x = self.embedding(x)\n",
        "    x = self.rnn(x)\n",
        "    return x\n",
        "\n",
        "  def convert_input(self,texts):\n",
        "    texts = tf.convert_to_tensor(texts)\n",
        "    if len(texts.shape) == 0:\n",
        "      texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
        "    context = self.text_processor(texts).to_tensor()\n",
        "    context = self(context)\n",
        "    return context"
      ],
      "metadata": {
        "id": "s9NbgeKC92jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now build the cross attention layer\n",
        "UNITS = 256\n",
        "\n",
        "class CrossAttention(layers.Layer):\n",
        "  def __init__(self,units,**kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = layers.MultiHeadAttention(key_dim=units,num_heads=1,**kwargs)\n",
        "    self.layernorm = layers.LayerNormalization()\n",
        "    self.add = layers.Add()\n",
        "\n",
        "  def call(self,x,context):\n",
        "    attn_output, attn_scores = self.mha(query=x,value=context,return_attention_scores=True)\n",
        "    attn_scores = tf.reduce_mean(attn_scores,axis=1)\n",
        "    self.last_attention_weights = attn_scores\n",
        "\n",
        "    x = self.add([x,attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "OMULSI97MQfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_layer = CrossAttention(UNITS)\n",
        "\n",
        "embed = layers.Embedding(target_text_processor.vocabulary_size(),output_dim=UNITS,mask_zero=True)"
      ],
      "metadata": {
        "id": "roBvGiJ8PbCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we build the decoder\n",
        "\n",
        "class Decoder(layers.Layer):\n",
        "  @classmethod\n",
        "  def add_method(cls,fun):\n",
        "    setattr(cls,fun.__name__,fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self,text_processor,units):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.text_processor = text_processor\n",
        "    self.vocab_size = text_processor.vocabulary_size()\n",
        "    self.word_to_id = layers.StringLookup(vocabulary=text_processor.get_vocabulary(), mask_token='',oov_token='[UNK]')\n",
        "    self.id_to_word = layers.StringLookup(vocabulary=text_processor.get_vocabulary(), mask_token='',oov_token='[UNK]',invert=True)\n",
        "    self.start_token = self.word_to_id('[START]')\n",
        "    self.end_token = self.word_to_id('[END]')\n",
        "    self.units = units\n",
        "\n",
        "    self.embedding = layers.Embedding(self.vocab_size,units,mask_zero=True)\n",
        "\n",
        "    self.rnn = layers.GRU(units,return_sequences=True,return_state = True, recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    self.attention = CrossAttention(units)\n",
        "\n",
        "    self.output_layer = layers.Dense(self.vocab_size)"
      ],
      "metadata": {
        "id": "GltzONshSKEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@Decoder.add_method\n",
        "def call(self,context,x,state=None,return_state=False):\n",
        "  x = self.embedding(x)\n",
        "  x , state = self.rnn(x, initial_state = state)\n",
        "  x = self.attention(x,context)\n",
        "  self.last_attention_weights = self.attention.last_attention_weights\n",
        "  logits = self.output_layer(x)\n",
        "  if return_state:\n",
        "    return logits, state\n",
        "  else:\n",
        "    return logits\n",
        "\n",
        "decoder = Decoder(target_text_processor,UNITS)"
      ],
      "metadata": {
        "id": "OlNyyhVVWYkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now build the model\n",
        "\n",
        "class Translator(keras.Model):\n",
        "  @classmethod\n",
        "  def add_method(cls,fun):\n",
        "    setattr(cls,fun.__name__,fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self,units,context_text_processor,target_text_processor):\n",
        "    super().__init__()\n",
        "\n",
        "    encoder = Encoder(context_text_processor,units)\n",
        "    decoder = Decoder(target_text_processor,units)\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def call(self,inputs):\n",
        "    context ,x = inputs\n",
        "    context = self.encoder(context)\n",
        "    logits = self.decoder(context,x)\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "bqF6zRrrYgFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Translator(UNITS,context_text_processor,target_text_processor)"
      ],
      "metadata": {
        "id": "jIAjZgcWjEE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_loss(y_true, y_pred):\n",
        "    loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "    loss = loss_fn(y_true, y_pred)\n",
        "    mask = tf.cast(y_true != 0, loss.dtype)\n",
        "    loss *= mask\n",
        "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "\n",
        "def masked_acc(y_true, y_pred):\n",
        "    y_pred = tf.argmax(y_pred, axis=-1)\n",
        "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
        "    matchs = tf.cast(y_true == y_pred, tf.float32)\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "    return tf.reduce_sum(matchs)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "r9sE3sKajYaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=masked_loss,\n",
        "              metrics=[masked_acc, masked_loss])"
      ],
      "metadata": {
        "id": "AKX2Ar7QkHBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_ds.repeat(),\n",
        "    epochs=50,\n",
        "    steps_per_epoch = 100,\n",
        "    validation_data=val_ds,\n",
        "    validation_steps = 20,\n",
        "    callbacks=[keras.callbacks.EarlyStopping(patience=3)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjd8cZLQkI4J",
        "outputId": "19205bad-ca3d-4572-b61d-4f20c6606f61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "100/100 [==============================] - 52s 230ms/step - loss: 5.3784 - masked_acc: 0.2391 - masked_loss: 5.3773 - val_loss: 4.4068 - val_masked_acc: 0.3277 - val_masked_loss: 4.4104\n",
            "Epoch 2/50\n",
            "100/100 [==============================] - 9s 93ms/step - loss: 3.9744 - masked_acc: 0.3765 - masked_loss: 3.9766 - val_loss: 3.6032 - val_masked_acc: 0.4185 - val_masked_loss: 3.6052\n",
            "Epoch 3/50\n",
            "100/100 [==============================] - 7s 68ms/step - loss: 3.3640 - masked_acc: 0.4573 - masked_loss: 3.3653 - val_loss: 3.1354 - val_masked_acc: 0.4803 - val_masked_loss: 3.1392\n",
            "Epoch 4/50\n",
            "100/100 [==============================] - 8s 80ms/step - loss: 2.9178 - masked_acc: 0.5173 - masked_loss: 2.9205 - val_loss: 2.7550 - val_masked_acc: 0.5402 - val_masked_loss: 2.7599\n",
            "Epoch 5/50\n",
            "100/100 [==============================] - 6s 62ms/step - loss: 2.5822 - masked_acc: 0.5695 - masked_loss: 2.5853 - val_loss: 2.3942 - val_masked_acc: 0.5947 - val_masked_loss: 2.3978\n",
            "Epoch 6/50\n",
            "100/100 [==============================] - 8s 85ms/step - loss: 2.3632 - masked_acc: 0.6024 - masked_loss: 2.3638 - val_loss: 2.2785 - val_masked_acc: 0.6052 - val_masked_loss: 2.2808\n",
            "Epoch 7/50\n",
            "100/100 [==============================] - 6s 62ms/step - loss: 2.1560 - masked_acc: 0.6334 - masked_loss: 2.1574 - val_loss: 2.0609 - val_masked_acc: 0.6366 - val_masked_loss: 2.0613\n",
            "Epoch 8/50\n",
            "100/100 [==============================] - 8s 78ms/step - loss: 2.0121 - masked_acc: 0.6531 - masked_loss: 2.0137 - val_loss: 1.9560 - val_masked_acc: 0.6484 - val_masked_loss: 1.9608\n",
            "Epoch 9/50\n",
            "100/100 [==============================] - 6s 60ms/step - loss: 1.9175 - masked_acc: 0.6639 - masked_loss: 1.9189 - val_loss: 1.7895 - val_masked_acc: 0.6820 - val_masked_loss: 1.7917\n",
            "Epoch 10/50\n",
            "100/100 [==============================] - 8s 77ms/step - loss: 1.8033 - masked_acc: 0.6824 - masked_loss: 1.8066 - val_loss: 1.7252 - val_masked_acc: 0.6936 - val_masked_loss: 1.7300\n",
            "Epoch 11/50\n",
            "100/100 [==============================] - 7s 68ms/step - loss: 1.7635 - masked_acc: 0.6867 - masked_loss: 1.7650 - val_loss: 1.6964 - val_masked_acc: 0.6896 - val_masked_loss: 1.6967\n",
            "Epoch 12/50\n",
            "100/100 [==============================] - 6s 61ms/step - loss: 1.6602 - masked_acc: 0.6999 - masked_loss: 1.6629 - val_loss: 1.6485 - val_masked_acc: 0.6951 - val_masked_loss: 1.6499\n",
            "Epoch 13/50\n",
            "100/100 [==============================] - 7s 71ms/step - loss: 1.6083 - masked_acc: 0.7061 - masked_loss: 1.6105 - val_loss: 1.5490 - val_masked_acc: 0.7157 - val_masked_loss: 1.5493\n",
            "Epoch 14/50\n",
            "100/100 [==============================] - 6s 61ms/step - loss: 1.5736 - masked_acc: 0.7097 - masked_loss: 1.5755 - val_loss: 1.4888 - val_masked_acc: 0.7196 - val_masked_loss: 1.4895\n",
            "Epoch 15/50\n",
            "100/100 [==============================] - 8s 79ms/step - loss: 1.4906 - masked_acc: 0.7204 - masked_loss: 1.4926 - val_loss: 1.5015 - val_masked_acc: 0.7150 - val_masked_loss: 1.5045\n",
            "Epoch 16/50\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 1.2051 - masked_acc: 0.7570 - masked_loss: 1.2052 - val_loss: 1.5194 - val_masked_acc: 0.7179 - val_masked_loss: 1.5203\n",
            "Epoch 17/50\n",
            "100/100 [==============================] - 7s 74ms/step - loss: 1.1947 - masked_acc: 0.7570 - masked_loss: 1.1964 - val_loss: 1.4371 - val_masked_acc: 0.7285 - val_masked_loss: 1.4389\n",
            "Epoch 18/50\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 1.1979 - masked_acc: 0.7550 - masked_loss: 1.1994 - val_loss: 1.4415 - val_masked_acc: 0.7278 - val_masked_loss: 1.4428\n",
            "Epoch 19/50\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 1.1978 - masked_acc: 0.7567 - masked_loss: 1.2003 - val_loss: 1.4617 - val_masked_acc: 0.7279 - val_masked_loss: 1.4620\n",
            "Epoch 20/50\n",
            "100/100 [==============================] - 7s 67ms/step - loss: 1.2126 - masked_acc: 0.7519 - masked_loss: 1.2140 - val_loss: 1.4087 - val_masked_acc: 0.7311 - val_masked_loss: 1.4100\n",
            "Epoch 21/50\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 1.1972 - masked_acc: 0.7542 - masked_loss: 1.1986 - val_loss: 1.3259 - val_masked_acc: 0.7427 - val_masked_loss: 1.3267\n",
            "Epoch 22/50\n",
            "100/100 [==============================] - 8s 77ms/step - loss: 1.1833 - masked_acc: 0.7594 - masked_loss: 1.1845 - val_loss: 1.3648 - val_masked_acc: 0.7338 - val_masked_loss: 1.3662\n",
            "Epoch 23/50\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 1.1678 - masked_acc: 0.7607 - masked_loss: 1.1701 - val_loss: 1.3330 - val_masked_acc: 0.7422 - val_masked_loss: 1.3338\n",
            "Epoch 24/50\n",
            "100/100 [==============================] - 7s 74ms/step - loss: 1.1585 - masked_acc: 0.7603 - masked_loss: 1.1588 - val_loss: 1.2677 - val_masked_acc: 0.7470 - val_masked_loss: 1.2661\n",
            "Epoch 25/50\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 1.1597 - masked_acc: 0.7634 - masked_loss: 1.1606 - val_loss: 1.2346 - val_masked_acc: 0.7517 - val_masked_loss: 1.2350\n",
            "Epoch 26/50\n",
            "100/100 [==============================] - 7s 68ms/step - loss: 1.1326 - masked_acc: 0.7673 - masked_loss: 1.1338 - val_loss: 1.3293 - val_masked_acc: 0.7397 - val_masked_loss: 1.3307\n",
            "Epoch 27/50\n",
            "100/100 [==============================] - 6s 64ms/step - loss: 1.1394 - masked_acc: 0.7668 - masked_loss: 1.1420 - val_loss: 1.2525 - val_masked_acc: 0.7529 - val_masked_loss: 1.2533\n",
            "Epoch 28/50\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 1.1253 - masked_acc: 0.7677 - masked_loss: 1.1264 - val_loss: 1.2468 - val_masked_acc: 0.7513 - val_masked_loss: 1.2512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hCIXqcgClQjy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}